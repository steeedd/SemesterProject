{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz-QXqByFGaE"
      },
      "source": [
        "# Create an `MPC` client and server with OS LLM\n",
        "\n",
        "In this tutorial we will create and **MCP server** that offers tools for weather forecasting and an **MCP client** that uses an Open Source LLM (`Qwen-2.5-7B-Instruct`) with **HuggingFace** `transformers`\n",
        "\n",
        "> Based on the Anthropic [example](https://modelcontextprotocol.io/introduction)\n",
        "\n",
        "> Created by [Manu Romero](http://twitter.com/mrm8488)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QuuEC96XDzX"
      },
      "source": [
        "## **1. Introduction to MCP**\n",
        "\n",
        "The Model Context Protocol (MCP) is an open standard developed by Anthropic that enables AI applications to interact seamlessly with external data sources and tools. By implementing MCP, AI models can access real-time information and perform tasks beyond their initial training data.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Setting Up the Environment**\n",
        "\n",
        "In this section, we'll set up the necessary environment by installing required libraries and configuring the runtime to support asynchronous operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLiktnJmX-nT"
      },
      "source": [
        "We will use an **L4** GPU, you can try with the free **T4** and maybe quantize the model to 4 bits (in this example we quantize it to 8 bits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zlf_Q-YHKR8Q"
      },
      "outputs": [],
      "source": [
        "# Apply nest_asyncio to allow nested use of asyncio.run in notebooks.\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVLh7tCAFuEl"
      },
      "source": [
        "## **3. Implementing the MCP Server**\n",
        "\n",
        "We'll create an MCP server that provides tools to fetch weather alerts and forecasts. This server will interact with external weather APIs and expose functionalities via MCP tools.\n",
        "\n",
        "**Key Components:**\n",
        "- **make_nws_request:** A helper function to make requests to the National Weather Service (NWS) API.\n",
        "- **format_alert:** A function to format weather alert data.\n",
        "- **get_alerts:** An MCP tool to retrieve active weather alerts for a given state.\n",
        "- **get_forecast:** An MCP tool to fetch weather forecasts for a specific location based on latitude and longitude.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLWkOu2rFPN2",
        "outputId": "697643ad-1829-4e00-eeda-78151c6f9bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting weather_mcp_server.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile weather_mcp_server.py\n",
        "\n",
        "from typing import Any\n",
        "import httpx\n",
        "from mcp.server.fastmcp import FastMCP\n",
        "import logging\n",
        "\n",
        "# Initialize FastMCP server\n",
        "mcp = FastMCP(\"weather\")\n",
        "\n",
        "# Constants\n",
        "NWS_API_BASE = \"https://api.weather.gov\"\n",
        "USER_AGENT = \"weather-app/1.0\"\n",
        "\n",
        "\n",
        "# Configura il logging su file\n",
        "logging.basicConfig(filename=\"weather_mcp_server.log\", level=logging.DEBUG)\n",
        "logging.debug(\"Server starting...\")\n",
        "logging.debug(\"Registered tools (before registration): %s\", list(mcp._tool.keys()))\n",
        "\n",
        "# Helper functions for formatting\n",
        "async def make_nws_request(url: str) -> dict[str, Any] | None:\n",
        "    \"\"\"Make a request to the NWS API with proper error handling.\"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": USER_AGENT,\n",
        "        \"Accept\": \"application/geo+json\"\n",
        "    }\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        try:\n",
        "            response = await client.get(url, headers=headers, timeout=30.0)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def format_alert(feature: dict) -> str:\n",
        "    \"\"\"Format an alert feature into a readable string.\"\"\"\n",
        "    props = feature[\"properties\"]\n",
        "    return f\"\"\"\n",
        "Event: {props.get('event', 'Unknown')}\n",
        "Area: {props.get('areaDesc', 'Unknown')}\n",
        "Severity: {props.get('severity', 'Unknown')}\n",
        "Description: {props.get('description', 'No description available')}\n",
        "Instructions: {props.get('instruction', 'No specific instructions provided')}\n",
        "\"\"\"\n",
        "\n",
        "# Implementing tool execution\n",
        "@mcp.tool()\n",
        "async def get_alerts(state: str) -> str:\n",
        "    \"\"\"Get weather alerts for a US state.\n",
        "\n",
        "    Args:\n",
        "        state: Two-letter US state code (e.g. CA, NY)\n",
        "    \"\"\"\n",
        "    url = f\"{NWS_API_BASE}/alerts/active/area/{state}\"\n",
        "    data = await make_nws_request(url)\n",
        "\n",
        "    if not data or \"features\" not in data:\n",
        "        return \"Unable to fetch alerts or no alerts found.\"\n",
        "\n",
        "    if not data[\"features\"]:\n",
        "        return \"No active alerts for this state.\"\n",
        "\n",
        "    alerts = [format_alert(feature) for feature in data[\"features\"]]\n",
        "    return \"\\n---\\n\".join(alerts)\n",
        "\n",
        "@mcp.tool()\n",
        "async def get_forecast(latitude: float, longitude: float) -> str:\n",
        "    \"\"\"Get weather forecast for a location.\n",
        "\n",
        "    Args:\n",
        "        latitude: Latitude of the location\n",
        "        longitude: Longitude of the location\n",
        "    \"\"\"\n",
        "    # First get the forecast grid endpoint\n",
        "    points_url = f\"{NWS_API_BASE}/points/{latitude},{longitude}\"\n",
        "    points_data = await make_nws_request(points_url)\n",
        "\n",
        "    if not points_data:\n",
        "        return \"Unable to fetch forecast data for this location.\"\n",
        "\n",
        "    # Get the forecast URL from the points response\n",
        "    forecast_url = points_data[\"properties\"][\"forecast\"]\n",
        "    forecast_data = await make_nws_request(forecast_url)\n",
        "\n",
        "    if not forecast_data:\n",
        "        return \"Unable to fetch detailed forecast.\"\n",
        "\n",
        "    # Format the periods into a readable forecast\n",
        "    periods = forecast_data[\"properties\"][\"periods\"]\n",
        "    forecasts = []\n",
        "    for period in periods[:5]:  # Only show next 5 periods\n",
        "        forecast = f\"\"\"\n",
        "{period['name']}:\n",
        "Temperature: {period['temperature']}°{period['temperatureUnit']}\n",
        "Wind: {period['windSpeed']} {period['windDirection']}\n",
        "Forecast: {period['detailedForecast']}\n",
        "\"\"\"\n",
        "        forecasts.append(forecast)\n",
        "\n",
        "    return \"\\n---\\n\".join(forecasts)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Initialize and run the server\n",
        "  mcp.run(transport='stdio')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8QMf9iRHK53"
      },
      "source": [
        "## **4. Implementing the MCP Client**\n",
        "\n",
        "We'll develop a client that connects to the MCP server and utilizes the tools provided. This client will use Anthropic's API to interact with the model and process user queries.\n",
        "\n",
        "**Key Components:**\n",
        "- **MCPClient Class:** Manages the connection to the MCP server, lists available tools, and facilitates interaction between the user and the server.\n",
        "- **connect_to_server:** Establishes a connection to the specified MCP server.\n",
        "- **list_tools:** Retrieves and displays the tools available on the connected server.\n",
        "- **chat_loop:** Handles the interactive loop where the user inputs queries, and the client processes them using the model and server tools.\n",
        "- **cleanup:** Ensures proper closure of the client session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHWyo3FaHwHg",
        "outputId": "37af2486-d137-4be1-f746-aae9b483a190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting mcp_client.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mcp_client.py\n",
        "\n",
        "import re\n",
        "import json\n",
        "import asyncio\n",
        "from typing import Optional\n",
        "from contextlib import AsyncExitStack\n",
        "import warnings\n",
        "\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Use there the model you wish (it must support tool calling)\n",
        "MODEL_ID = \"Qwen/Qwen3-1.7B\"\n",
        "\n",
        "\n",
        "# Load the model and tokenizer (quantized to 8bit)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Helper function becaus in transformers the tool calls should be a field of assistant messages.\n",
        "def try_parse_tool_calls(content: str):\n",
        "    \"\"\"Try parse the tool calls.\"\"\"\n",
        "    tool_calls = []\n",
        "    offset = 0\n",
        "    for i, m in enumerate(re.finditer(r\"<tool_call>\\n(.+)?\\n</tool_call>\", content)):\n",
        "        if i == 0:\n",
        "            offset = m.start()\n",
        "        try:\n",
        "            func = json.loads(m.group(1))\n",
        "            tool_calls.append({\"type\": \"function\", \"function\": func})\n",
        "            if isinstance(func[\"arguments\"], str):\n",
        "                func[\"arguments\"] = json.loads(func[\"arguments\"])\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Failed to parse tool calls: the content is {m.group(1)} and {e}\")\n",
        "            pass\n",
        "    if tool_calls:\n",
        "        if offset > 0 and content[:offset].strip():\n",
        "            c = content[:offset]\n",
        "        else:\n",
        "            c = \"\"\n",
        "        return {\"role\": \"assistant\", \"content\": c, \"tool_calls\": tool_calls}\n",
        "    return {\"role\": \"assistant\", \"content\": re.sub(r\"<\\|im_end\\|>$\", \"\", content)}\n",
        "\n",
        "\n",
        "class MCPClient:\n",
        "    def __init__(self):\n",
        "        # Initialize session and client objects\n",
        "        self.session: Optional[ClientSession] = None\n",
        "        self.exit_stack = AsyncExitStack()\n",
        "        self.llm = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    async def connect_to_server(self, server_script_path: str):\n",
        "        \"\"\"Connect to an MCP server\n",
        "\n",
        "        Args:\n",
        "            server_script_path: Path to the server script (.py or .js)\n",
        "        \"\"\"\n",
        "        is_python = server_script_path.endswith('.py')\n",
        "        is_js = server_script_path.endswith('.js')\n",
        "        if not (is_python or is_js):\n",
        "            raise ValueError(\"Server script must be a .py or .js file\")\n",
        "\n",
        "        command = \"python\" if is_python else \"node\"\n",
        "        server_params = StdioServerParameters(\n",
        "            command=command,\n",
        "            args=[server_script_path],\n",
        "            env=None\n",
        "        )\n",
        "\n",
        "        print(\"DEBUG: Before stdio_client\")\n",
        "        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
        "        print(\"DEBUG: After stdio_client\")\n",
        "        self.stdio, self.write = stdio_transport\n",
        "        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n",
        "\n",
        "        await self.session.initialize()\n",
        "\n",
        "        # List available tools\n",
        "        response = await self.session.list_tools()\n",
        "        print(\"TOOLS:\", [t.name for t in response.tools])\n",
        "        tools = response.tools\n",
        "        print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
        "\n",
        "    async def process_query(self, query: str) -> str:\n",
        "      \"\"\"Process a query using Claude and available tools\"\"\"\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": query\n",
        "          }\n",
        "      ]\n",
        "\n",
        "      response = await self.session.list_tools()\n",
        "      available_tools = [{\n",
        "          \"name\": tool.name,\n",
        "          \"description\": tool.description,\n",
        "          \"input_schema\": tool.inputSchema\n",
        "      } for tool in response.tools]\n",
        "\n",
        "\n",
        "      # Initial LLM Call\n",
        "      text = self.tokenizer.apply_chat_template(messages, tools=available_tools, add_generation_prompt=True, tokenize=False)\n",
        "      print(\"PROMPT SENT TO LLM:\\n\", text)\n",
        "      inputs = self.tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "      outputs = self.llm.generate(**inputs, max_new_tokens=512)\n",
        "      output_text = tokenizer.batch_decode(outputs)[0][len(text):]\n",
        "      print(\"OUTPUT LLM:\\n\", output_text)\n",
        "\n",
        "\n",
        "      # Processing response and handel tool calls with the LLM `output_text`\n",
        "      final_text = []\n",
        "\n",
        "      parsed_message = try_parse_tool_calls(output_text)\n",
        "      messages.append(parsed_message)\n",
        "\n",
        "      final_text.append(parsed_message[\"content\"])\n",
        "\n",
        "      if tool_calls := messages[-1].get(\"tool_calls\", None):\n",
        "          for tool_call in tool_calls:\n",
        "              if fn_call := tool_call.get(\"function\"):\n",
        "                  fn_name: str = fn_call[\"name\"]\n",
        "                  fn_args: dict = fn_call[\"arguments\"]\n",
        "\n",
        "                  print(f\"Calling tool: {fn_name} with args: {fn_args}\")\n",
        "                  final_text.append(f\"Calling tool: {fn_name} with args: {fn_args}\")\n",
        "                  result = await self.session.call_tool(fn_name, fn_args)\n",
        "                  #print(result)\n",
        "                  fn_res = result.content\n",
        "                  #print(f\"Tool result: {fn_res}\")\n",
        "\n",
        "                  messages.append({\n",
        "                      \"role\": \"tool\",\n",
        "                      \"name\": fn_name,\n",
        "                      \"content\": fn_res,\n",
        "                  })\n",
        "\n",
        "              # Get next response from Claude\n",
        "              text = self.tokenizer.apply_chat_template(messages, tools=available_tools, add_generation_prompt=True, tokenize=False)\n",
        "              inputs = self.tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "              outputs = self.llm.generate(**inputs, max_new_tokens=512)\n",
        "              output_text = self.tokenizer.batch_decode(outputs)[0][len(text):]\n",
        "\n",
        "              final_text.append(output_text)\n",
        "\n",
        "\n",
        "\n",
        "      return \"\\n\".join(final_text)\n",
        "\n",
        "    async def chat_loop(self):\n",
        "      \"\"\"Run an interactive chat loop\"\"\"\n",
        "      print(\"\\nMCP Client Started!\")\n",
        "      print(\"Type your queries or 'quit' to exit.\")\n",
        "\n",
        "      while True:\n",
        "          try:\n",
        "              query = \"What is the weather in San Francisco today?\"\n",
        "\n",
        "              if query.lower() == 'quit':\n",
        "                  break\n",
        "\n",
        "              response = await self.process_query(query)\n",
        "              print(\"\\n\" + response)\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"\\nError: {str(e)}\")\n",
        "\n",
        "    async def cleanup(self):\n",
        "      \"\"\"Clean up resources\"\"\"\n",
        "      await self.exit_stack.aclose()\n",
        "\n",
        "\n",
        "async def main():\n",
        "    client = MCPClient()\n",
        "    try:\n",
        "        # Connetti al server MCP\n",
        "        await client.connect_to_server(\"weather_mcp_server.py\")\n",
        "\n",
        "\n",
        "        # Dai il prompt una sola volta\n",
        "        user_prompt = \"What is the weather in San Francisco today?\"\n",
        "        response = await client.process_query(user_prompt)\n",
        "\n",
        "\n",
        "        # Stampa la risposta finale\n",
        "        print(\"\\nLLM + Tool Calling Response:\\n\", response)\n",
        "\n",
        "    finally:\n",
        "        await client.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2wQiNr5JG44"
      },
      "source": [
        "## Runing it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFfjvqpHJRe7",
        "outputId": "8c9ea78d-bf02-46dc-8697-d0f8d0bdd957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.51s/it]\n",
            "Some parameters are on the meta device because they were offloaded to the disk.\n",
            "DEBUG: Before stdio_client\n",
            "DEBUG: After stdio_client\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/weather_mcp_server.py\"\u001b[0m, line \u001b[35m18\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    logging.debug(\"Registered tools (before registration): %s\", list(\u001b[1;31mmcp._tool\u001b[0m.keys()))\n",
            "                                                                     \u001b[1;31m^^^^^^^^^\u001b[0m\n",
            "\u001b[1;35mAttributeError\u001b[0m: \u001b[35m'FastMCP' object has no attribute '_tool'. Did you mean: 'tool'?\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/mcp_client.py\"\u001b[0m, line \u001b[35m200\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    \u001b[31masyncio.run\u001b[0m\u001b[1;31m(main())\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\"\u001b[0m, line \u001b[35m194\u001b[0m, in \u001b[35mrun\u001b[0m\n",
            "    return \u001b[31mrunner.run\u001b[0m\u001b[1;31m(main)\u001b[0m\n",
            "           \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\"\u001b[0m, line \u001b[35m118\u001b[0m, in \u001b[35mrun\u001b[0m\n",
            "    return \u001b[31mself._loop.run_until_complete\u001b[0m\u001b[1;31m(task)\u001b[0m\n",
            "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m720\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
            "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/mcp_client.py\"\u001b[0m, line \u001b[35m184\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "    await client.connect_to_server(\"weather_mcp_server.py\")\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/mcp_client.py\"\u001b[0m, line \u001b[35m84\u001b[0m, in \u001b[35mconnect_to_server\u001b[0m\n",
            "    await self.session.initialize()\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/mcp/client/session.py\"\u001b[0m, line \u001b[35m152\u001b[0m, in \u001b[35minitialize\u001b[0m\n",
            "    result = \u001b[1;31mawait self.send_request(\u001b[0m\n",
            "             \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "    ...<15 lines>...\n",
            "    \u001b[1;31m)\u001b[0m\n",
            "    \u001b[1;31m^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/mcp/shared/session.py\"\u001b[0m, line \u001b[35m288\u001b[0m, in \u001b[35msend_request\u001b[0m\n",
            "    raise McpError(response_or_error.error)\n",
            "\u001b[1;35mmcp.shared.exceptions.McpError\u001b[0m: \u001b[35mConnection closed\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! python mcp_client.py weather_mcp_server.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
