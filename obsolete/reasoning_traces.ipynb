{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eR5ecZTOU5-k",
        "outputId": "32adf81b-7bc1-4359-a507-86e0a670169e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vllm\n",
            "  Downloading vllm-0.11.2.tar.gz (17.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\\^C\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/bin/pip\"\u001b[0m, line \u001b[35m10\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
            "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/pip/_internal/cli/main.py\"\u001b[0m, line \u001b[35m80\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "    return \u001b[31mcommand.main\u001b[0m\u001b[1;31m(cmd_args)\u001b[0m\n",
            "           \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\"\u001b[0m, line \u001b[35m156\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "    with \u001b[31mself.main_context\u001b[0m\u001b[1;31m()\u001b[0m:\n",
            "         \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m148\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    \u001b[31mnext\u001b[0m\u001b[1;31m(self.gen)\u001b[0m\n",
            "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/pip/_internal/cli/command_context.py\"\u001b[0m, line \u001b[35m19\u001b[0m, in \u001b[35mmain_context\u001b[0m\n",
            "    with \u001b[1;31mself._main_context\u001b[0m:\n",
            "         \u001b[1;31m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m619\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    raise exc\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m604\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    if \u001b[31mcb\u001b[0m\u001b[1;31m(*exc_details)\u001b[0m:\n",
            "       \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m148\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    \u001b[31mnext\u001b[0m\u001b[1;31m(self.gen)\u001b[0m\n",
            "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m42\u001b[0m, in \u001b[35mglobal_tempdir_manager\u001b[0m\n",
            "    with \u001b[31mExitStack\u001b[0m\u001b[1;31m()\u001b[0m as stack:\n",
            "         \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m619\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    raise exc\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m604\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    if \u001b[31mcb\u001b[0m\u001b[1;31m(*exc_details)\u001b[0m:\n",
            "       \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m169\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    \u001b[31mself.cleanup\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m212\u001b[0m, in \u001b[35mcleanup\u001b[0m\n",
            "    \u001b[31mrmtree\u001b[0m\u001b[1;31m(self._path, ignore_errors=False)\u001b[0m\n",
            "    \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/pip/_internal/utils/retry.py\"\u001b[0m, line \u001b[35m34\u001b[0m, in \u001b[35mretry_wrapped\u001b[0m\n",
            "    return func(*args, **kwargs)\n",
            "  File \u001b[35m\"/Users/stefan/Desktop/Eurecom/Semester Project/venv/lib/python3.13/site-packages/pip/_internal/utils/misc.py\"\u001b[0m, line \u001b[35m135\u001b[0m, in \u001b[35mrmtree\u001b[0m\n",
            "    \u001b[31mshutil.rmtree\u001b[0m\u001b[1;31m(dir, onexc=handler)\u001b[0m  # type: ignore\n",
            "    \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/shutil.py\"\u001b[0m, line \u001b[35m763\u001b[0m, in \u001b[35mrmtree\u001b[0m\n",
            "    \u001b[31m_rmtree_safe_fd\u001b[0m\u001b[1;31m(stack, onexc)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/shutil.py\"\u001b[0m, line \u001b[35m696\u001b[0m, in \u001b[35m_rmtree_safe_fd\u001b[0m\n",
            "    \u001b[31mos.unlink\u001b[0m\u001b[1;31m(entry.name, dir_fd=topfd)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install vllm transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ELGjhedVO85",
        "outputId": "d33d6daa-382f-4215-e12d-c0182a14efaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: litellm in /usr/local/lib/python3.12/dist-packages (1.79.1)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm) (8.2.1)\n",
            "Requirement already satisfied: fastuuid>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.14.0)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (4.25.1)\n",
            "Requirement already satisfied: openai>=1.99.5 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.109.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (2.11.10)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.2.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.12.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm) (0.22.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.28.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.4)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install litellm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A4ZrChFVUu8",
        "outputId": "bed52e18-c782-4dd0-e320-ca25cc15601d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "!nohup vllm serve Qwen/Qwen3-1.7B \\\n",
        "  --enable-auto-tool-choice \\\n",
        "  --tool-call-parser hermes \\\n",
        "  --reasoning-parser deepseek_r1 \\\n",
        "  > vllm.log &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6flAyxuVXNR",
        "outputId": "fa0fc978-8779-462c-badd-fa30a43941fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/cli_args.py\", line 263, in make_arg_parser\n",
            "    parser = AsyncEngineArgs.add_cli_args(parser)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 1714, in add_cli_args\n",
            "    parser = EngineArgs.add_cli_args(parser)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 919, in add_cli_args\n",
            "    vllm_kwargs = get_kwargs(VllmConfig)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 281, in get_kwargs\n",
            "    return copy.deepcopy(_compute_kwargs(cls))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 182, in _compute_kwargs\n",
            "    default = field.default_factory()\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_dataclasses.py\", line 123, in __init__\n",
            "    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/config/device.py\", line 58, in __post_init__\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to infer device type, please set the environment variable `VLLM_LOGGING_LEVEL=DEBUG` to turn on verbose logging to help debug the issue.\n"
          ]
        }
      ],
      "source": [
        "!tail -n 20 vllm.log # to watch last 20 rows og vllm.log, about 2/3 minutes to have the server ready -> Application startup complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYiocefoVZOe"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "def query_db(sql: str):\n",
        "    \"\"\"\n",
        "    Executes an SQL query and returns the result as JSON\n",
        "\n",
        "    Args:\n",
        "        sql: The SQL query to execute\n",
        "\n",
        "    Returns:\n",
        "        The result of the query as JSON\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_sql_query(sql, conn)\n",
        "        return df.to_dict(orient=\"records\")\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE9DUNVmVtqj"
      },
      "outputs": [],
      "source": [
        "# Dopo aver creato la tabella 'employees', aggiungi questo:\n",
        "conn = sqlite3.connect(\"company.db\") # Assicurati che conn sia attiva\n",
        "\n",
        "# Modifica i dati 'employees' per includere 'dept_id'\n",
        "data_employees = {\n",
        "    \"id\": [1, 2, 3],\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
        "    \"hire_date\": [\"2020-01-15\", \"2021-06-20\", \"2022-03-10\"],\n",
        "    \"salary\": [50000, 60000, 70000],\n",
        "    \"dept_id\": [1, 2, 1] # Alice e Charlie sono nel Dip. 1, Bob nel 2\n",
        "}\n",
        "df_employees = pd.DataFrame(data_employees)\n",
        "df_employees.to_sql(\"employees\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "# Crea dati 'departments'\n",
        "data_departments = {\n",
        "    \"dept_id\": [1, 2],\n",
        "    \"dept_name\": [\"Engineering\", \"Marketing\"]\n",
        "}\n",
        "df_departments = pd.DataFrame(data_departments)\n",
        "df_departments.to_sql(\"departments\", conn, if_exists=\"replace\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4V0nwSXWBLT"
      },
      "outputs": [],
      "source": [
        "def get_function_by_name(name):\n",
        "    if name == \"query_db\":\n",
        "        return query_db\n",
        "\n",
        "TOOLS = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"query_db\",\n",
        "            \"description\": \"Executes a single SQL query on the database. Use this to explore the schema (e.g., PRAGMA table_info) or to get the final answer.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"sql\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": 'The SQL query to execute on the database.',\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"sql\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD1J1wMQXK1C"
      },
      "outputs": [],
      "source": [
        "USER_QUESTION = \"Find the name of the highest-paid employee and also list their department's name.\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert SQL assistant. Your task is to answer complex user questions by generating a final SQL query.\n",
        "\n",
        "You do not know the database schema in advance.\n",
        "\n",
        "You must follow a strict multi-step process:\n",
        "1.  **Initial Analysis:** Think (in the reasoning/thinking trace) about what you need to discover.\n",
        "2.  **Exploration (Loop):** You must first explore the database.\n",
        "    * Use `SELECT name FROM sqlite_master WHERE type='table'` to find the tables.\n",
        "    * Use `PRAGMA table_info(table_name)` to understand a table's columns.\n",
        "    * Use `SELECT * FROM table_name LIMIT 3` to see sample data, if necessary.\n",
        "    * Use the `query_db` tool for *EVERY SINGLE* exploratory query.\n",
        "3.  **Intermediate Reasoning:** After each call to `query_db`, analyze the result and explain (in the reasoning trace) what you have learned and what your next exploratory step will be.\n",
        "4.  **Final Construction:** Only when you are *absolutely sure* you have all the information (table names, column names, data types, keys for JOINs), build the final SQL query that answers the user's question.\n",
        "5.  **Final Answer:** Execute the final query with `query_db` and provide the answer to the user in natural language, based on the result.\n",
        "\n",
        "Do not attempt to answer the user's question with a single query before you have explored the schema.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRIeqb4-XIXf"
      },
      "outputs": [],
      "source": [
        "from litellm import completion\n",
        "\n",
        "model_name = \"hosted_vllm/Qwen/Qwen3-1.7B\"\n",
        "MAX_TURNS = 6 # Imposta un limite per evitare loop infiniti\n",
        "\n",
        "# Inizializza la cronologia della conversazione\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": USER_QUESTION}\n",
        "]\n",
        "\n",
        "# Parametri comuni per 'completion'\n",
        "completion_params = {\n",
        "    \"model\": model_name,\n",
        "    \"temperature\": 0.1, # Bassa temperatura per ragionamenti piÃ¹ logici\n",
        "    \"top_p\": 0.8,\n",
        "    \"max_tokens\": 1024, # Aumenta per trace piÃ¹ lunghe\n",
        "    \"repetition_penalty\": 1.05,\n",
        "    \"extra_body\": {\n",
        "        \"chat_template_kwargs\": {\n",
        "            \"enable_thinking\": True,\n",
        "            \"detailed_thinking\": True\n",
        "        }\n",
        "    },\n",
        "    \"api_base\": \"http://127.0.0.1:8000/v1\",\n",
        "    \"tools\": TOOLS\n",
        "}\n",
        "\n",
        "print(f\"ğŸš€ Inizio Ragionamento per: '{USER_QUESTION}'\\n\")\n",
        "\n",
        "for i in range(MAX_TURNS):\n",
        "    print(f\"\\n--- ğŸ”„ Turno {i+1} ---\")\n",
        "\n",
        "    # 1. Chiama il modello\n",
        "    response = completion(messages=messages, **completion_params)\n",
        "    response_message = response.choices[0].message\n",
        "\n",
        "    # Aggiungi la risposta del modello (pensiero + tool_call) alla cronologia\n",
        "    messages.append(response_message.model_dump())\n",
        "\n",
        "    # Stampa la \"traccia di ragionamento\" (se il modello la fornisce)\n",
        "    # Nota: la traccia di vLLM potrebbe essere nel log o in un campo non standard\n",
        "    # Controlliamo 'content' se c'Ã¨ pensiero prima del tool_call\n",
        "    if response_message.content:\n",
        "         print(f\"ğŸ¤” Pensiero del Modello:\\n{response_message.content}\")\n",
        "\n",
        "    # 2. Controlla se il modello ha chiamato uno strumento\n",
        "    if tool_calls := response_message.get(\"tool_calls\", None):\n",
        "        print(\"ğŸ› ï¸ Chiamata Strumento Rilevata:\")\n",
        "\n",
        "        # 3. Esegui tutti i tool_calls\n",
        "        for tool_call in tool_calls:\n",
        "            call_id = tool_call[\"id\"]\n",
        "            if fn_call := tool_call.get(\"function\"):\n",
        "                fn_name = fn_call[\"name\"]\n",
        "                fn_args = json.loads(fn_call[\"arguments\"])\n",
        "                sql_query = fn_args.get(\"sql\", \"\")\n",
        "\n",
        "                print(f\"   -> Esecuzione: {sql_query}\")\n",
        "\n",
        "                # Esegui la funzione\n",
        "                fn_res = get_function_by_name(fn_name)(**fn_args)\n",
        "                print(f\"   <- Risultato DB: {fn_res}\")\n",
        "\n",
        "                # 4. Aggiungi il risultato dello strumento alla cronologia\n",
        "                messages.append({\n",
        "                    \"role\": \"tool\",\n",
        "                    \"content\": json.dumps({\n",
        "                        \"query_eseguita\": sql_query,\n",
        "                        \"risultato\": fn_res\n",
        "                    }),\n",
        "                    \"tool_call_id\": call_id,\n",
        "                })\n",
        "\n",
        "    # 5. Controlla se il modello ha finito (ha dato una risposta finale senza chiamare strumenti)\n",
        "    elif response_message.content:\n",
        "        print(f\"\\nâœ… Risposta Finale del Modello:\\n{response_message.content}\")\n",
        "        break # Il ciclo Ã¨ finito\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ Il modello non ha fornito nÃ© 'content' nÃ© 'tool_calls'. Interrompo.\")\n",
        "        break\n",
        "\n",
        "if i == MAX_TURNS - 1:\n",
        "    print(\"\\nğŸš« Limite turni raggiunto. Il modello non ha completato il ragionamento.\")\n",
        "\n",
        "# Chiudi la connessione al DB\n",
        "conn.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
